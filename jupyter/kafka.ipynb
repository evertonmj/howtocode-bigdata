{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evertonmj/howtocode_bigdata/blob/main/kafka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlg5HqSI6fPM",
        "outputId": "b3f66f63-bb21-4f07-9870-1fa10e6e0c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IBhqsoaJ6sO7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fhpYOvPeLv2q"
      },
      "outputs": [],
      "source": [
        "kafka_version='kafka_2.12-3.8.0'\n",
        "topic_name = 'reco-train'\n",
        "kafka_bootstrap_servers = 'localhost:9092'\n",
        "kafka = kafka_version + '/bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBHTJ1kr6ysF",
        "outputId": "1948a12b-5da7-47cf-9f5a-dbc355528b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Career Mode female player datasets - FIFA 16-22.xlsx'\n",
            "'Career Mode player datasets - FIFA 15-22.xlsx'\n",
            " female_players_16.csv\n",
            " female_players_17.csv\n",
            " female_players_18.csv\n",
            " female_players_19.csv\n",
            " female_players_20.csv\n",
            " female_players_21.csv\n",
            " female_players_22.csv\n",
            " fifa-22-complete-player-dataset.zip\n",
            " kafka_2.12-3.8.0\n",
            " kafka_2.12-3.8.0.tgz\n",
            " players_15.csv\n",
            " players_16.csv\n",
            " players_17.csv\n",
            " players_18.csv\n",
            " players_19.csv\n",
            " players_20.csv\n",
            " players_21.csv\n",
            " players_22.csv\n",
            " sample_data\n",
            " spark-3.5.2-bin-hadoop3\n",
            " spark-3.5.2-bin-hadoop3.tgz\n",
            " spark-3.5.2-bin-hadoop3.tgz.1\n",
            " spark-3.5.2-bin-hadoop3.tgz.2\n",
            " spark-3.5.2-bin-hadoop3.tgz.3\n",
            " spark-streaming-kafka-0-10-assembly_2.13-3.5.2.jar\n",
            " spark-streaming-kafka-0-10-assembly_2.13-3.5.2.jar.1\n",
            " spark-streaming-kafka-0-10-assembly_2.13-3.5.2.jar.2\n",
            " spark-streaming-kafka-0-10-assembly_2.13-3.5.2.jar.3\n",
            " spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!curl -sSOL https://downloads.apache.org/kafka/3.8.0/{kafka_version}.tgz\n",
        "!tar -xzf {kafka_version}.tgz\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgJCVvEn61Wy",
        "outputId": "84827b15-19dc-4005-fa57-93e1a7f67ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin  config  libs  LICENSE  licenses  logs  NOTICE  site-docs\n",
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n"
          ]
        }
      ],
      "source": [
        "!ls {kafka_version}\n",
        "!./{kafka_version}/bin/zookeeper-server-start.sh -daemon ./{kafka_version}/config/zookeeper.properties\n",
        "!./{kafka_version}/bin/kafka-server-start.sh -daemon ./{kafka_version}/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qmRbxfd66Pt",
        "outputId": "712b4898-5c84-4534-b98c-f2e6cd6340df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root      109373  108245  0 02:05 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root      109375  109373  0 02:05 ?        00:00:00 grep kafka\n"
          ]
        }
      ],
      "source": [
        "!ps -ef | grep kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cX220RX8ubp",
        "outputId": "ea49cfa7-9b4f-4050-bf9a-f6f75e669b00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error while executing topic command : Topic 'reco-train' already exists.\n",
            "[2024-09-05 02:05:11,237] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'reco-train' already exists.\n",
            " (org.apache.kafka.tools.TopicCommand)\n",
            "Error while executing topic command : Topic 'reco-test' already exists.\n",
            "[2024-09-05 02:05:14,697] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'reco-test' already exists.\n",
            " (org.apache.kafka.tools.TopicCommand)\n"
          ]
        }
      ],
      "source": [
        "!./{kafka_version}/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic reco-train\n",
        "!./{kafka_version}/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 2 --topic reco-test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVmAd2Bb86V1"
      },
      "outputs": [],
      "source": [
        "!./{kafka_version}/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic reco-train\n",
        "!./{kafka_version}/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic reco-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGWS4e379Dla"
      },
      "outputs": [],
      "source": [
        "# !pip install kagle\n",
        "!kaggle datasets download -d stefanoleone992/fifa-22-complete-player-dataset\n",
        "# !wget -O ml_ratings.csv https://github.com/sparsh-ai/reco-data/blob/master/MovieLens_100K_ratings.csv?raw=true\n",
        "!unzip fifa-22-complete-player-dataset.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC9uja6_-SPa"
      },
      "outputs": [],
      "source": [
        "players_df_15 = pd.read_csv('players_15.csv')\n",
        "# players_df_16 = pd.read_csv('players_16.csv')\n",
        "# players_df_17 = pd.read_csv('players_17.csv')\n",
        "# players_df_18 = pd.read_csv('players_18.csv')\n",
        "# players_df_19 = pd.read_csv('players_19.csv')\n",
        "# players_df_20 = pd.read_csv('players_20.csv')\n",
        "# players_df_21 = pd.read_csv('players_21.csv')\n",
        "# players_df_22 = pd.read_csv('players_22.csv')\n",
        "# players_df = pd.concat([players_df_15, players_df_16, players_df_17, players_df_18, players_df_19, players_df_20, players_df_21, players_df_22])\n",
        "players_df = players_df_15\n",
        "players_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geO6asNF-nhB"
      },
      "outputs": [],
      "source": [
        "len(players_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uxBdIS_f--45"
      },
      "outputs": [],
      "source": [
        "players_df.columns.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38vfpyNX_5Ws"
      },
      "outputs": [],
      "source": [
        "!pip install kafka-python\n",
        "\n",
        "import json\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "def error_callback(exc):\n",
        "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
        "\n",
        "def write_to_kafka(topic_name, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],\n",
        "                         key_serializer=lambda k: json.dumps(k).encode(),\n",
        "                         value_serializer=lambda v: json.dumps(v).encode()) # Changed the key serializer to convert the tuple key to bytes\n",
        "  for message, key in items:\n",
        "    producer.send(topic_name, key=key, value=message).add_errback(error_callback)\n",
        "    count+=1\n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9DpY3pyAWks"
      },
      "outputs": [],
      "source": [
        "# records = players_df.to_json(orient='records', lines=True)\n",
        "# players_df = players_df.set_index(columns=players_df.columns['sofifa_id'])\n",
        "# players_df.set_index('sofifa_id', inplace=True)\n",
        "# records = players_df.set_index('sofifa_id')[players_df.columns.values.toList()].apply(tuple, axis=1).to_dict()\n",
        "# records = players_df.to_dict('index')\n",
        "players_df = players_df[~players_df.index.duplicated(keep='first')]\n",
        "\n",
        "players_df = players_df.reset_index().set_index(['sofifa_id', 'short_name'])\n",
        "records = players_df.to_dict('index')\n",
        "\n",
        "\n",
        "# records = list(players_df.itertuples(index=True, name=None))\n",
        "# print(records[:10])\n",
        "# records = json.loads(records)\n",
        "# records\n",
        "write_to_kafka(\"reco-train\", records.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEwsjFAehsEZ"
      },
      "outputs": [],
      "source": [
        "!pip install confluent-kafka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4LFFOpXiTOb"
      },
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer, TopicPartition\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "consumer = Consumer({\"bootstrap.servers\": \"localhost:9092\", \"group.id\": \"reco-train\"})\n",
        "\n",
        "def get_partition_size(topic_name: str, partition_key: int):\n",
        "    topic_partition = TopicPartition(topic_name, partition_key)\n",
        "    low_offset, high_offset = consumer.get_watermark_offsets(topic_partition)\n",
        "    partition_size = high_offset - low_offset\n",
        "    return partition_size\n",
        "\n",
        "def get_topic_size(topic_name: str):\n",
        "    topic = consumer.list_topics(topic=topic_name)\n",
        "    partitions = topic.topics[topic_name].partitions\n",
        "    workers, max_workers = [], len(partitions) or 1\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as e:\n",
        "        for partition_key in list(topic.topics[topic_name].partitions.keys()):\n",
        "            job = e.submit(get_partition_size, topic_name, partition_key)\n",
        "            workers.append(job)\n",
        "\n",
        "    topic_size = sum([w.result() for w in workers])\n",
        "    return topic_size\n",
        "\n",
        "print(get_topic_size('reco-train'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYOfhk5nK7FU"
      },
      "outputs": [],
      "source": [
        "# !./{kafka_version}/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic recoTraing --from-beginning --max-messages 100\n",
        "!./{kafka_version}/bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic reco-train --group=reco-train --from-beginning --max-messages 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rxv9UC5jp-4"
      },
      "outputs": [],
      "source": [
        "print(get_topic_size('reco-train'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhf4W_cKmj5L"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
        "!tar -xvf spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip install findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhcYDNj9moBj"
      },
      "outputs": [],
      "source": [
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10-assembly_2.13/3.5.2/spark-streaming-kafka-0-10-assembly_2.13-3.5.2.jar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEnyuTd_msW0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.2-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-10-assembly_2.13-3.5.2.jar pyspark-shell'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRXmxfFWKryX"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuJxuZc4mtoU"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stsYOfD-mw26"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Normalizer, StandardScaler\n",
        "import random\n",
        "import pyspark\n",
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "# from pyspark.streaming.kafka import KafkaUtils\n",
        "from uuid import uuid1\n",
        "import time\n",
        "\n",
        "kafka_topic_name = \"reco-train\"\n",
        "kafka_bootstrap_servers = 'localhost:9092'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall pyspark==2.4.6\n"
      ],
      "metadata": {
        "id": "rDqWbxVz64hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.2\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Now you can import and use KafkaUtils\n",
        "from pyspark.streaming.kafka import KafkaUtils"
      ],
      "metadata": {
        "id": "b9TRfK5LxlB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print(\"Current Time =\", current_time)"
      ],
      "metadata": {
        "id": "ZaIBaKA9xS77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = pyspark.SparkContext()\n",
        "ssc = StreamingContext(sc,5)\n",
        "\n",
        "kafka_topic_name = \"reco-train\"\n",
        "kafka_bootstrap_servers = 'localhost:9092'\n",
        "\n",
        "kvs = KafkaUtils.createStream(ssc, kafka_bootstrap_servers, 'spark-streaming-consumer', {kafka_topic_name:1})\n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\"metadata.broker.list\": kafka_bootstrap_servers})\n",
        "kvs = KafkaUtils.createDirectStream(ssc, [kafka_topic_name], {\n",
        "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
        "                        'group.id':'test-group',\n",
        "                        'auto.offset.reset':'largest'})\n",
        "\n",
        "lines = kvs.map(lambda x: x[1])\n",
        "counts = lines.flatMap(lambda line: line.split(' '))\n",
        "counts = lines.flatMap(lambda line: line.split(' ')).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
        "counts.pprint()\n",
        "ssc.start()\n",
        "# stream will run for 50 sec\n",
        "ssc.awaitTerminationOrTimeout(50)\n",
        "ssc.stop()\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "vgNq6LbjxYTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP48AfxN84Vu0ELdKVGE2V2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}